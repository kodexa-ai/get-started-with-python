{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sinks, Document Stores, and \n",
    "\n",
    "Sinks and document stores are used to capture Kodexa documents as they pass through pipelines and can be used to save the processed Kodexa documents in memory or on disk.\n",
    "\n",
    "When performing processess that result in multiple documents\n",
    "\n",
    "\n",
    "## Setup our imports\n",
    "1. We'll be build pipelines to process our document, so we'll import Kodexa's Pipeline module\n",
    "2. Import all the connector types that we plan to try out\n",
    "3. Some of our parsing will need to happen in the could, so we'll import the KodexaPlatform and KodexaAction modules\n",
    "4. All files that have been processed/parsed in Kodexa become Kodexa Documents, so we'll import that module as well.\n",
    "\n",
    "We're also setting the CLOUD_URL value to the platform environment on which we want to perform our processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kodexa import Document, Pipeline, KodexaPlatform, KodexaAction, InMemoryDocumentSink, JsonDocumentStore, TableDataStore, DictDataStore\n",
    "\n",
    "CLOUD_URL = 'https://platform.kodexa.com' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Platform Environment and Access Token Credential\n",
    "\n",
    "In the next cell, you'll be prompted to enter your access token that you've created in the environment specified by the CLOUD_URL.\n",
    "If you haven't created a token already, follow the steps in our [Getting Started](https://developer.kodexa.com/org-management/manage-access-token) guide.\n",
    "\n",
    "* Note:  The text you enter in the prompt field will be masked.  Once you're done entering the access token value, hit enter to complete the action in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter access token: ································\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "ACCESS_TOKEN = getpass.getpass(\"Enter access token:\")\n",
    "\n",
    "KodexaPlatform.set_url(CLOUD_URL)\n",
    "KodexaPlatform.set_access_token(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setting up location of data folders and files\n",
    "DATA_FOLDER = '_data'\n",
    "TEXT_FOLDER = 'texts'\n",
    "JSON_STORE_FOLDER = 'json_doc_stores'\n",
    "TEXT_DATA_FILE = 'tongue_twister.txt'\n",
    "\n",
    "TEXT_FOLDER_PATH = os.path.join(os.getcwd(), '..', DATA_FOLDER, TEXT_FOLDER)\n",
    "FULL_PATH = os.path.join(TEXT_FOLDER_PATH, TEXT_DATA_FILE)\n",
    "JSON_STORE_PATH = os.path.join(os.getcwd(), '..', DATA_FOLDER, JSON_STORE_FOLDER, 'text_json_store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an InMemoryDocumentSink\n",
    "\n",
    "In many of our other examples where we're only processing one document, we don't provide a sink, but instead\n",
    "retrieve the last processed document from the pipeline's context (pipeline.context.output_document).  If you're only\n",
    "processing one document and choose to provide a sink, the document avaiable from that sink and the document available\n",
    "on the pipeline's context are the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the document on the pipeline's context the same as that in the sink?\n",
      "\n",
      "Yes!  They are the same!\n",
      "\tfaae1970-ba4f-5436-b4e4-88dfc001d971\n",
      "\tfaae1970-ba4f-5436-b4e4-88dfc001d971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sink = InMemoryDocumentSink()\n",
    "\n",
    "pipeline = Pipeline.from_text(\"Well hello there!  It's so nice to see you!\")\n",
    "pipeline.set_sink(sink)\n",
    "pipeline.run()\n",
    "\n",
    "kodexa_doc = sink.get_document(0)\n",
    "same_doc = pipeline.context.output_document\n",
    "\n",
    "print('Is the document on the pipeline\\'s context the same as that in the sink?\\n')\n",
    "\n",
    "if kodexa_doc.uuid == same_doc.uuid:\n",
    "    print('Yes!  They are the same!')\n",
    "else:\n",
    "    print('Nope - totally different documents.\\n')\n",
    "    \n",
    "print(f'\\t{kodexa_doc.uuid}')\n",
    "print(f'\\t{same_doc.uuid}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the InMemoryDocumentSink to hold multiple documents\n",
    "\n",
    "In most of the Getting Started examples, we're processing only one document at a time.  This example\n",
    "demonstrates that you can read all of files from a folder, process them with the same pipeline actions,\n",
    "and return all of them in the pipeline's sink.\n",
    "\n",
    "Here, we read all of the *.txt files from our sample data folder and verify that they are both available on the\n",
    "sink.  We can also see that the pipeline's context only contains the last document processsed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 documents in the sink.\n",
      "Is the document on the pipeline's context the same as the last one in the sink?\n",
      "\n",
      "Yes!  They are the same!\n",
      "\t8228120f-666a-4255-ab91-e37c702b07d1\n",
      "\t8228120f-666a-4255-ab91-e37c702b07d1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kodexa import FolderConnector\n",
    "\n",
    "sink = InMemoryDocumentSink()\n",
    "pipeline = Pipeline(FolderConnector(path=TEXT_FOLDER_PATH, file_filter='*.txt'))\n",
    "pipeline.add_step(KodexaAction(slug='kodexa/text-parser', options={}, attach_source=True))\n",
    "pipeline.set_sink(sink)\n",
    "\n",
    "# Run the pipeline and get the pipeline's context.  We'll then get the last processed document from the context\n",
    "pipeline.run()\n",
    "context_kodexa_doc = pipeline.context.output_document\n",
    "\n",
    "# check the number of documents returned on the sink\n",
    "print(f'There are {len(sink.documents)} documents in the sink.')\n",
    "\n",
    "print('Is the document on the pipeline\\'s context the same as the last one in the sink?\\n')\n",
    "\n",
    "if context_kodexa_doc.uuid == sink.get_document(1).uuid:\n",
    "    print('Yes!  They are the same!')\n",
    "else:\n",
    "    print('Nope - totally different documents.\\n')\n",
    "    \n",
    "print(f'\\t{context_kodexa_doc.uuid}')\n",
    "print(f'\\t{sink.get_document(1).uuid}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the JsonDocumentStore\n",
    "\n",
    "The JsonDocumentStore allows you to store parsed or processed documents and can be used as a connector, a sink, or as a stand-alone saved file.  When persisted to disk, the JsonDocumentStore is saved in JSON format.\n",
    "\n",
    "If you want to delete stored documents in an existing, populated JsonDocumentStore, use the force_initialize parameter.  It will remove all of the documents within the store and clear out the index of document ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The JsonDocumentStore as a sink\n",
    "\n",
    "In this example, we'll use the JsonDocumentStore in the same way we used the InMemoryDocumentSink. \n",
    "We'll read all of the text (*.txt) files from our sample data and verify the store contains our documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 documents in the sink.\n",
      "Is the document on the pipeline's context the same as the last one in the sink?\n",
      "\n",
      "Yes!  They are the same!\n",
      "\tdac2ba48-f28d-4e9f-b786-14c950cc7d37\n",
      "\tdac2ba48-f28d-4e9f-b786-14c950cc7d37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# instantiate the store and provide the path to which we'd want to persist the file.\n",
    "# Passing in force_initialize as TRUE to ensure any old data is overwritten\n",
    "json_doc_store = JsonDocumentStore(store_path=JSON_STORE_PATH, force_initialize=True)\n",
    "\n",
    "pipeline = Pipeline(FolderConnector(path=TEXT_FOLDER_PATH, file_filter='*.txt'))\n",
    "pipeline.add_step(KodexaAction(slug='kodexa/text-parser', options={}, attach_source=True))\n",
    "pipeline.set_sink(json_doc_store)\n",
    "\n",
    "# Run the pipeline and get the pipeline's context.  We'll then get the last processed document from the context\n",
    "pipeline.run()\n",
    "context_kodexa_doc = pipeline.context.output_document\n",
    "\n",
    "# check the number of documents returned on the sink\n",
    "print(f'There are {json_doc_store.count()} documents in the store.')\n",
    "\n",
    "print('Is the document on the pipeline\\'s context the same as the last one in the sink?\\n')\n",
    "\n",
    "if context_kodexa_doc.uuid == json_doc_store.get_document(1).uuid:\n",
    "    print('Yes!  They are the same!')\n",
    "else:\n",
    "    print('Nope - totally different documents.\\n')\n",
    "\n",
    "print(f'\\t{context_kodexa_doc.uuid}')\n",
    "print(f'\\t{json_doc_store.get_document(1).uuid}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View JsonDocumentStore output\n",
    "\n",
    "Once you've processed the pipeline above, you will be able to view the folder containing the JsonDocumentStore and inspect the output.  You'll find a file named index.json which contains the uuids for each of the documents in the store, as well as a file for each processed Kodexa document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the JsonDocumentStore as a Connector\n",
    "\n",
    "With our text files parsed and saved in the JSON_STORE_PATH, we can now construct a new pipeline and read those documents.\n",
    "Since the documents were parsed prior to them being stored in the JsonDocumentStore, we won't have to parse them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 documents in the connector.\n",
      "There are 2 documents in the sink.\n",
      "Is the document on the pipeline's context the same as the last one in the sink?\n",
      "\n",
      "Yes!  They are the same!\n",
      "\tdac2ba48-f28d-4e9f-b786-14c950cc7d37\n",
      "\tdac2ba48-f28d-4e9f-b786-14c950cc7d37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a connector using the JsonDocumentStore with the path to which we've already written our ouput\n",
    "connector = JsonDocumentStore(store_path=JSON_STORE_PATH)\n",
    "\n",
    "# how many documents are in the store?\n",
    "print(f'There are {connector.count()} documents in the connector.')\n",
    "\n",
    "# Since our JsonDocumentStore contains more than one document, we'll need a sink to access all of them.\n",
    "sink = InMemoryDocumentSink()\n",
    "\n",
    "\n",
    "pipeline = Pipeline(connector)\n",
    "pipeline.set_sink(sink)\n",
    "pipeline.run()\n",
    "\n",
    "# check the number of documents returned on the sink\n",
    "print(f'There are {len(sink.documents)} documents in the sink.')\n",
    "\n",
    "print('Is the document on the pipeline\\'s context the same as the last one in the sink?\\n')\n",
    "\n",
    "if context_kodexa_doc.uuid == sink.get_document(1).uuid:\n",
    "    print('Yes!  They are the same!')\n",
    "else:\n",
    "    print('Nope - totally different documents.\\n')\n",
    "    \n",
    "print(f'\\t{context_kodexa_doc.uuid}')\n",
    "print(f'\\t{sink.get_document(1).uuid}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a TableDataStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a DictDataStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo coming soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kodexa_demos",
   "language": "python",
   "name": "kodexa_demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
